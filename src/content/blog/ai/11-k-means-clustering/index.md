---
title: "11. K-Means Clustering Explained"
summary: ""
date: "Oct 11 2025"
draft: false
pinned: false
tags:
- AI
---


Welcome back to **ML Experts â€” Machine Learning Crash Course**!
In our previous lessons, we explored **supervised learning**, where models learned from labeled examples â€” â€œthis is spamâ€ vs â€œthis is not spam,â€ or â€œpower outageâ€ vs â€œno outage.â€

But what if we donâ€™t have any labels at all? What if we just have raw data and want to **discover structure** within it?
Thatâ€™s where **unsupervised learning** comes in â€” and one of the most popular algorithms here is **K-Means Clustering**.

---

## ğŸŒ The Scenario: Understanding Team Blindâ€™s Users

Imagine weâ€™ve been hired by **Team Blind**, the anonymous workplace social app.
People sign up under their company name and post workplace discussions anonymously.

Our goal?
To **grow the user base** â€” but we know nothing about who the users are or how they behave.

All we have are behavioral metrics:

* ğŸ‘ Likes given and received per day
* â± Average session time
* ğŸ“ Posts created per month
* ğŸ“œ Feed scrolling time per session
* ğŸ’¬ Direct messages sent or received per day

We donâ€™t know which users are â€œreadersâ€ or â€œwritersâ€ â€” just these raw numbers.
So, how can we make sense of this data?
Enter **K-Means**.

---

## ğŸ§© The Core Idea: Finding â€œKâ€ Averages

The name **K-Means** says it all:
It finds **K clusters (groups)** by locating **K mean points** â€” or **centroids** â€” that represent the center of each group.

Letâ€™s say we choose **K = 2**.
We start by placing **two random centroids** in our dataset.
Each data point is assigned to the **nearest centroid** based on **distance** (usually Euclidean distance).

Then, we move each centroid to the **average position** of all points in its cluster.
We repeat this process â€” assign â†’ update â†’ assign â†’ update â€” until the centroids **stop moving**.
At that point, the algorithm has **converged**.

---

## ğŸ“‰ Measuring Cluster Quality: Inertia

How do we know if our clusters make sense?
We measure something called **inertia** â€” the **sum of distances** between each point and its clusterâ€™s centroid.

A smaller inertia means the points are tightly grouped â€” and thatâ€™s what we want.
The goal of K-Means is to **minimize inertia**.

---

## âš™ï¸ Efficiency and the Role of Lloydâ€™s Algorithm

If we tried every possible clustering combination, the computation would explode to **O(N^(KÂ·D + 1))** â€” totally impractical.

Instead, K-Means uses **Lloydâ€™s algorithm**, which brings complexity down to **O(NÂ·KÂ·D)**.
But thereâ€™s a trade-off:
Itâ€™s only an **approximation**, so it might get stuck in a **local minimum** instead of the **global best** solution.

This happens because the inertia function is **non-convex** â€” there can be multiple â€œvalleys,â€ and the algorithm might settle in the wrong one.

---

## ğŸŒ€ Getting Stuck in Local Minima

Letâ€™s say weâ€™re clustering with **K = 3**, but our random initialization places two centroids too close to each other.
The algorithm might end up grouping all nearby points under those two centroids â€” missing a better configuration elsewhere.
This is an example of **local minima**.

To mitigate this, we can:

* Use **K-Means++**, which chooses better initial centroids.
* Or use **Bisected K-Means**, which starts with two clusters, then recursively splits the one with the **highest inertia** until a good number of clusters emerge.

---

## ğŸ“Š Choosing the Right â€œKâ€

How do we know how many clusters we should have?
There are two popular methods:

### 1. The Elbow Method

We run K-Means with K = 1, 2, 3, â€¦, and plot the **inertia**.
As K increases, inertia decreases â€” but after a point, the improvement slows down.
That bend or â€œelbowâ€ in the curve is the **optimal K**.

### 2. The Silhouette Method

For each point:

* Compute **A(i)** = average distance to points in the same cluster
* Compute **B(i)** = average distance to points in the nearest other cluster
  Then compute the **Silhouette Coefficient**:
  [
  S(i) = \frac{B(i) - A(i)}{\max(A(i), B(i))}
  ]
  A higher average S(i) across all points means better separation.
  The K with the **highest silhouette score** is ideal.

---

## ğŸ“ˆ From Insight to Action: Growing Team Blindâ€™s User Base

After running K-Means, suppose we get **three clusters**:

1. **Content Consumers** â€“ Scroll a lot, like posts frequently.
2. **Content Creators** â€“ Post often, engage less with others.
3. **Hybrid Users** â€“ Occasionally write but also actively read.

By comparing cluster features, we can build **growth strategies**:

* ğŸ§­ **Encourage readers to write:** Send prompts about trending workplace topics or events.
* ğŸ¤– **Use bots to motivate new writers:** Offer encouragement or highlight engagement.
* ğŸš€ **Promote promising creators:** Push their posts to more readers to boost feedback.
* ğŸ’° **Offer referral rewards:** Bring in new users similar to existing active ones.

The key is to maintain **balance** â€” too many readers and too few writers will make the platform feel empty.

---

## ğŸ§® Beyond K-Means: Variants & Alternatives

| Algorithm     | Key Difference                         | Distance Metric       |
| ------------- | -------------------------------------- | --------------------- |
| **K-Medians** | Uses the **median** instead of mean    | Manhattan Distance    |
| **K-Medoids** | Centroid must be a **real data point** | Any (often Manhattan) |
| **K-Modes**   | Works with **categorical data**        | Dissimilarity measure |

However, some data shapes â€” like **circular rings** â€” confuse K-Means, which prefers convex clusters.
For those cases, we can use **Agglomerative Clustering**, which merges the nearest points step by step, building a **hierarchical structure**.

We can measure inter-cluster distance in different ways:

* **Single linkage:** Minimum distance between points in clusters.
* **Complete linkage:** Maximum distance between points in clusters.

---

## ğŸ§° Practical Notes

* ğŸ“¦ Use **scikit-learn**â€™s `KMeans` (which implements K-Means++) or `AgglomerativeClustering`.
* âš¡ For big data, use **Sparkâ€™s Bisected K-Means**.
* âš–ï¸ Always **standardize features** before clustering â€” subtract the mean and divide by the standard deviation.
* ğŸš« Watch out for **high-dimensional data** â€” it can reduce distance-based clustering accuracy due to the â€œcurse of dimensionality.â€

---

## ğŸ“ Final Thoughts

K-Means is simple, intuitive, and surprisingly powerful.
Even without labels, it can uncover meaningful patterns â€” like different types of users, customers, or behaviors â€” and guide strategic decisions.

In our Team Blind example, K-Means transformed anonymous clickstream data into **actionable insights** on how to **grow the platform sustainably**.

In the next lesson, weâ€™ll dive deeper into other unsupervised learning techniques â€” stay tuned for more!

