---
title: "2. Prerequisites for Machine Learning Crash Course"
summary: ""
date: "Oct 2 2025"
draft: false
pinned: false
tags:
- AI
---

Before we dive into building and understanding machine learning models, itâ€™s important to make sure we share a common foundation. This post outlines the key prerequisites youâ€™ll need â€” not in an intimidating â€œmath-onlyâ€ way, but in a practical sense: what you should understand *and why it matters.*

If your goal is simply to become **conversational in ML** â€” understanding the concepts at a high level â€” you can skim this.
But if youâ€™re aiming to **interview for technical ML roles**, then these fundamentals will make a huge difference in how easily you grasp model behavior, optimization, and performance.

---

### ğŸ§© Understanding the Building Blocks: Features, Labels & Examples

In machine learning, **features** are the input signals that describe your data â€” the information the model uses to make predictions.

For example, imagine weâ€™re predicting the price movement of Bitcoin.

* **Continuous feature:** The actual price of Bitcoin each month (e.g., $29,300 in Jan 2021, $33,500 in Feb, etc.).
* **Categorical feature:** Whether the market is â€œupâ€ or â€œdown.â€
* **Ordinal feature:** Categories that have a natural order, like *small â†’ medium â†’ large*.

Features are always paired with **labels**, which are what we want to predict. In supervised learning, each example in the dataset has both:

```
Example = (Features, Label)
```

If we remove the labels, weâ€™re in **unsupervised learning** territory â€” where the model must find structure on its own (like clustering similar data points).

---

### ğŸ”¢ Math Foundations: Vectors, Matrices & Operations

You donâ€™t need to be a math wizard to understand ML, but having a basic grasp of linear algebra helps you interpret whatâ€™s going on inside models.

* A **vector** is just an ordered list of numbers â€” like `[29.3k, 33.5k, 58.7k]`.
  In notation, youâ€™ll often see it written as **xÌ„ = [xâ‚, xâ‚‚, xâ‚ƒ, â€¦]**.
* When we plot two elements of a vector (say, `xâ‚` vs. `xâ‚‚`), each pair forms a **point** in space.

If you stack multiple vectors together, you get a **matrix**.
Matrices are useful for representing data in tabular form â€” rows for examples, columns for features.

Common matrix operations:

* **Multiplication (AB):** Combining two matrices to compute transformations.
* **Inverse (Aâ»Â¹):** Reversing a transformation.
* **Transpose (Aáµ€):** Flipping rows and columns.

While you wonâ€™t be doing heavy algebra by hand, ML frameworks like TensorFlow or PyTorch rely on these operations constantly under the hood.

---

### ğŸ“ˆ Polynomials & Derivatives

Occasionally, weâ€™ll encounter **polynomials**, especially when visualizing loss curves or fitting regression lines.
For instance:

* A **line** is a first-degree polynomial.
* A **quadratic curve** (parabola) is a second-degree polynomial.

To optimize models, we often need to take **derivatives** â€” which tell us how a function is changing. In ML, derivatives (or gradients) help us adjust model parameters in the right direction during training.
Donâ€™t worry if calculus isnâ€™t your strong suit; weâ€™ll revisit this when we discuss gradient descent.

---

### ğŸ² Probability Essentials

Probability helps quantify uncertainty â€” and machine learning thrives on that.

Letâ€™s start simple:

* The chance of rolling a â€œ2â€ on a six-sided die = **1/6**.
* The chance of rolling *two twos in a row* = **(1/6) Ã— (1/6) = 1/36**.

This is the **â€œandâ€** condition â€” both events must happen.

#### ğŸ” Conditional Probability

Suppose you draw two cards from a deck *without replacement*.

* First draw (heart): **13/52**
* Second draw (heart again): **12/51**

The combined probability:
**(13/52) Ã— (12/51) â‰ˆ 5.88%**

Conditional probability helps us model situations where one event affects the next â€” crucial for sequential data and Bayesian models.

---

### ğŸ“Š Distributions: How Uncertainty Is Shaped

Different kinds of data follow different statistical patterns, called **distributions**.

1. **Gaussian (Normal) Distribution**
   Most natural measurements â€” like human height â€” cluster around a mean.
   For example, if the average height is 63 inches, being 60 or 66 inches tall is slightly less likely, while being 50 or 75 inches is rare.

2. **Uniform Distribution**
   Every outcome is equally likely â€” just like rolling a fair die (each side = 1/6 probability).

3. **Beta Distribution**
   Used to model **rates or probabilities**, such as a websiteâ€™s conversion rate.
   If half your users click an ad, the most likely conversion rate is 50%, but it could fluctuate slightly (say, 25% or 75%) depending on new data.
   The beta distribution helps capture that uncertainty and how it narrows as more data accumulates.

---

### ğŸš€ Wrapping Up

Thatâ€™s your mathematical and statistical toolkit for this course.
Donâ€™t stress about memorizing every formula â€” the goal is to develop *intuition*. Think of these concepts as lenses through which we can interpret and improve our models.

In the next lessons, weâ€™ll move from these foundations to actual model-building â€” where these ideas will come alive through examples and code.

