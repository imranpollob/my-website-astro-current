---
title: "Understanding Linear Regression"
summary: ""
date: "Nov 2 2025"
draft: false
pinned: false
tags:
- AI
---

Imagine you have a scatterplot of points that seem to follow a general trend â€” as one variable increases, so does another. You could draw a line that best summarizes this pattern. That line represents the **relationship** between the two variables.

This, at its core, is **linear regression** â€” finding the *line of best fit* through data so that we can make predictions about future or unseen values.

---

### âš¡ Real-World Example: Predicting Power Demand

Letâ€™s say weâ€™ve just been hired by **California ISO**, the organization that manages Californiaâ€™s electric power grid. They want to **predict the power demand** for each region a day in advance.

Why?
If demand is expected to spike, they can increase supply; if demand is expected to fall, they can divert power elsewhere.

We hypothesize that **temperature** is a key driver: as the temperature rises in summer, more people turn on their air conditioners â€” increasing power demand.

So:

* **Feature (X):** Average daily temperature (from the National Weather Service)
* **Label (Y):** Daily power demand per region (in megawatts)

When we plot these data points, a clear pattern emerges â€” higher temperature, higher power usage.
Our goal: find a line that captures this trend.

---

### ðŸ“ˆ The Equation of a Line

The general form of a line is:

[
Y = mX + b
]

Where:

* **X** = input (temperature)
* **Y** = output (predicted power demand)
* **m** = slope (how much Y changes for a unit change in X)
* **b** = intercept (where the line crosses the Y-axis)

This allows us to plug in any temperature and get an **estimated power demand** for that region.

---

### ðŸ§® Finding the Line of Best Fit

To calculate the best line, we use a formula based on all the (X, Y) pairs.
The slope (**A**) and intercept (**B**) are derived from the averages and deviations of X and Y:

[
A = \frac{\sum (X_i - \bar{X})(Y_i - \bar{Y})}{\sum (X_i - \bar{X})^2}
]
[
B = \bar{Y} - A \bar{X}
]

After calculation, we might find:
**A = 4105**, **B = â€“50962**

So, the model becomes:
[
Y = 4105X - 50962
]

If tomorrowâ€™s temperature is predicted to be **25.2Â°C**, plugging that into the equation gives:
[
Y = 52,484 \text{ megawatts}
]

Thatâ€™s our **predicted power demand** for tomorrow.

---

### ðŸ“Š Interpreting the Coefficient

The slope (4105) means:

> For every **1Â°C increase in temperature**, power demand increases by **4105 MW**.

Thatâ€™s a strong and meaningful relationship â€” but how do we know itâ€™s not just random noise?

---

### âœ… Statistical Significance: P-Value

The **P-value** tells us whether our observed relationship is likely due to chance.

* If **P â‰¤ 0.05**, we consider the result *statistically significant*.
* In our case, **P = 0.0009**, which means thereâ€™s less than a 0.1% chance the relationship is random.

Hence, we can confidently say: temperature and power demand are related.

---

### ðŸŽ¯ Confidence Intervals

Because our data only covers specific years and months, our coefficient is an *estimate* of the true, underlying relationship.

A **95% confidence interval** between **3100** and **5000** means:

> Thereâ€™s a 95% chance that the *true* increase in demand per Â°C lies between 3100 and 5000 MW.

If a confidence interval includes zero, the relationship isnâ€™t statistically meaningful.

---

### ðŸ”— Correlation vs. Causation

The **correlation coefficient (R)** measures how strongly two variables move together:

* **R = 1** â†’ Perfect positive correlation
* **R = â€“1** â†’ Perfect negative correlation
* **R = 0** â†’ No correlation

In our example, **R = 0.99**, which is excellent â€” temperature and demand rise together.
But remember, correlation â‰  causation. Just because they move together doesnâ€™t mean one causes the other.

---

### ðŸ§¾ R-Squared â€” How Well the Line Fits

We can also compute **RÂ²**, which tells us how much of the variation in Y is explained by X.

If **RÂ² = 0.98**, it means 98% of the variation in power demand is explained by temperature â€” leaving only 2% unexplained (random noise or other factors).

Adding more relevant features can increase RÂ², but beware of overfitting.

---

### ðŸ§© Expanding the Model: Multiple Regression

Real-world systems rarely depend on a single factor.
We can extend our model to include more **independent variables**, such as:

* Region population size (small, medium, large)
* Region type (industrial, commercial, residential)
* Humidity, etc.

This becomes a **multiple linear regression**:

[
Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + ... + \beta_n X_n
]

For categorical data like â€œregion type,â€ we use **one-hot encoding** â€” converting each category into binary variables (1 or 0).
For example:

* Industrial â†’ [1, 0]
* Residential â†’ [0, 1]
* Commercial â†’ [0, 0]

---

### ðŸ” Detecting Collinearity

If two features (e.g., temperature and humidity) are highly correlated, it creates **multicollinearity**, which distorts the interpretation of coefficients.

We detect this using the **Variance Inflation Factor (VIF):**

* **VIF = 1:** No collinearity
* **1â€“5:** Moderate, acceptable
* **>5:** High â€” needs fixing (by centering, removing features, or combining them)

---

### âš™ï¸ Feature Interactions and Nonlinearity

Sometimes features interact â€” for example, â€œregion typeâ€ might change how temperature affects power demand.
We can model this by **multiplying features together** (interaction terms).

We can also include **nonlinear terms** (like XÂ², XÂ³, or log(X)) to fit curves instead of straight lines.
But beware: adding too many terms can lead to **overfitting** â€” where your model memorizes noise instead of learning the pattern.

---

### âš ï¸ Simpsonâ€™s Paradox â€” The Hidden Trap

Suppose we ignore the â€œregion sizeâ€ feature and combine all regions into one dataset.
We might end up with a misleading line that suggests:

> As temperature increases, power demand *decreases.*

Thatâ€™s completely false!
This is known as **Simpsonâ€™s Paradox** â€” a situation where aggregated data hides or reverses true relationships.
The cure? Always segment data properly and include all relevant contextual features.

---

### ðŸ§° Tools of the Trade

A great Python library for this is **StatsModels** â€” it handles:

* Multiple regression
* P-values
* Confidence intervals
* RÂ² and adjusted RÂ²
* Variance Inflation Factors (VIF)

Under the hood, libraries donâ€™t use the â€œclosed-formâ€ NÂ³ regression formula we derived.
Instead, they rely on more efficient techniques like **Singular Value Decomposition (SVD)**.

---

### ðŸ§© Summary

| Concept                 | Key Idea                                    |
| ----------------------- | ------------------------------------------- |
| **Linear Regression**   | Fits a line to predict Y from X             |
| **Slope (m)**           | Change in Y for each unit increase in X     |
| **P-Value**             | Probability relationship is due to chance   |
| **Confidence Interval** | Range of likely true coefficient values     |
| **RÂ²**                  | Fraction of variance explained by the model |
| **VIF**                 | Detects feature correlation (collinearity)  |
| **Simpsonâ€™s Paradox**   | Misleading conclusions from aggregated data |

---

### ðŸŒ… Wrapping Up

Linear regression may seem simple, but itâ€™s one of the most **powerful and interpretable models** in data science.
It builds the foundation for many advanced techniques â€” from generalized linear models to deep learning regressors.

As you move forward, remember:

> A well-fitted line can reveal powerful insights â€” but only when you understand what the data truly represents.

Stay tuned for the next episode in the ML Experts Crash Course as we continue exploring the fascinating world of machine learning!

