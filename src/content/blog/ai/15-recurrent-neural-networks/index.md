---
title: "Understanding Recurrent Neural Networks (RNNs)"
summary: ""
date: "Nov 2 2025"
draft: false
pinned: false
tags:
- AI
---


Welcome back to **ML Expertsâ€™ Machine Learning Crash Course!**
In todayâ€™s episode, weâ€™re diving into a key architecture in deep learning â€” the **Recurrent Neural Network (RNN)**.

If youâ€™ve ever wondered how chatbots can hold a conversation or how your phone predicts the next word in a sentence, this is the magic behind it.

---

## ğŸ’¬ The Chatbot Challenge

Imagine weâ€™re building a chatbot for the **TeamBlind app**, designed to message *first-time writers* and motivate them to continue writing.

* A user posts an article.
* Our chatbot sends a friendly DM.
* The goal is to encourage a conversation â€” and eventually turn casual posters into active content creators.

But thereâ€™s a problemâ€¦

Messages come in **different lengths**.
Some people write, â€œThanks,â€ while others type long paragraphs.
How can a neural network handle text of variable size?

One naÃ¯ve idea is to **fix message length** â€” say, 20 words â€” and *pad shorter ones with zeros*.

This works, but itâ€™s clunky. It ignores the natural *flow* of language.

Enter the **Recurrent Neural Network** â€” a model that remembers context, one word at a time.

---

## ğŸ” Why â€œRecurrentâ€?

In a regular neural network, each input is treated independently.
But in human language, each word *depends* on the words before it.

For example, the meaning of â€œbankâ€ changes depending on whether we say â€œriver bankâ€ or â€œsavings bank.â€

RNNs solve this by introducing a **loop**:
they take the previous output (or *hidden state*) and feed it back into the network as part of the next input.

This allows RNNs to **retain memory over sequences** â€” much like a person remembering the previous sentence in a conversation.

---

## ğŸ§© The Core Idea: Hidden States

Letâ€™s simplify how an RNN processes a message.

1. Each word is converted to a number (its position in the vocabulary).
   Example:

   * â€œThanksâ€ â†’ 32
   * â€œforâ€ â†’ 14
   * â€œreachingâ€ â†’ 53
   * â€œoutâ€ â†’ 9

2. These numbers are fed one-by-one into the RNN.

3. The RNN produces an output and a **hidden state** â€” a sort of *memory vector* that carries information from previous words.

At each step, this hidden state is updated and passed forward.
By the time the last word is processed, the network has built a *contextual understanding* of the whole message.

---

## ğŸ§® Backpropagation Through Time (BPTT)

Training an RNN is like training a regular neural network â€” but with a twist.
Since the network unfolds across time steps, we must â€œbackpropagateâ€ through each step.

This is known as **Backpropagation Through Time (BPTT)**.
Each time step contributes to the overall loss, and the gradients flow backward through all previous states.

However, this introduces a major issueâ€¦

---

## âš ï¸ The Vanishing Gradient Problem

As we move backward through many time steps, gradients can become **very small**, causing earlier words to be â€œforgotten.â€
In long sentences, the network struggles to connect early context (â€œIf it rains tomorrowâ€¦â€) with later predictions (â€œâ€¦bring an umbrellaâ€).

To overcome this, researchers developed a more advanced type of RNN:

---

## ğŸ§± Long Short-Term Memory (LSTM)

LSTMs add *gates* â€” mechanisms that control what information to **remember**, **forget**, or **output**.

Letâ€™s break it down:

| Gate               | Purpose                                  | Function                                                |
| ------------------ | ---------------------------------------- | ------------------------------------------------------- |
| ğŸ§¹ **Forget Gate** | Decides what old information to discard. | Uses a sigmoid layer (0â€“1 scale) to partially forget.   |
| â• **Input Gate**   | Decides what new information to store.   | Combines sigmoid (for control) and tanh (for new info). |
| ğŸ”“ **Output Gate** | Decides what part of memory to output.   | Regulates final hidden state for the next time step.    |

These gates make LSTMs far more **memory-stable**, allowing them to capture long-term dependencies.

---

## ğŸ§© Variations on LSTMs

1. **Stacked LSTMs**
   Multiple LSTM layers stacked on top of each other to capture hierarchical language patterns.

2. **Bidirectional LSTMs**
   Two LSTMs â€” one reads the sentence forward, the other backward â€” allowing the model to see *past* and *future* context simultaneously.

3. **GRUs (Gated Recurrent Units)**
   A simplified version of LSTMs with fewer gates and faster training â€” perfect for smaller datasets.

---

## ğŸŒ From Words to Embeddings

Feeding raw word indices to a network isnâ€™t ideal.
Instead, we can use **embeddings** â€” dense vector representations of words that capture meaning.

For instance:

* â€œkingâ€ â€“ â€œmanâ€ + â€œwomanâ€ â‰ˆ â€œqueenâ€

These embeddings are learned through **gradient descent** or can be preloaded from pretrained models like **Word2Vec** or **GloVe**.

In our chatbot, each word is embedded into a **10-dimensional vector space**, making relationships between words more meaningful to the model.

---

## ğŸªœ Making Predictions Efficient: Hierarchical Softmax

When your vocabulary grows (say, 10,000 words), computing probabilities for every possible word becomes expensive.

The **hierarchical softmax** speeds this up by organizing words in a tree.
Instead of computing probabilities for *every* word, it traverses the tree, reducing complexity from *O(V)* to *O(log V)*.

---

## ğŸ§  Building the Chatbot Model

Hereâ€™s the architecture we used for TeamBlindâ€™s chatbot:

* **Embedding layer:** 317 words â†’ 10 dimensions
* **Stacked LSTMs:**

  * First LSTM: 32 units
  * Second LSTM: 64 units
* **Output:** Standard Softmax for word prediction

ğŸ“Š **Result:**
11% of first-time writers became active, consistent writers within 30 days â€” a solid success!

---

## ğŸ§° Libraries & Training Tips

If youâ€™re implementing this yourself, start with **Keras** or **TensorFlow**.

ğŸ’¡ **Tips for stable training:**

* Initialize LSTM weights carefully to avoid degeneracy.
* Apply **dropout** for regularization.
* Use **gradient clipping** to avoid exploding gradients.
* Experiment with **embedding sizes**, **sequence lengths**, and **batch normalization**.

---

## ğŸ”® Wrapping Up

Recurrent Neural Networks revolutionized sequence modeling â€” from chatbots to language translation and speech recognition.
While vanilla RNNs struggle with long-term memory, **LSTMs** and **GRUs** elegantly solve those challenges.

The next time you see your phone completing your sentence or a chatbot replying intelligently, remember â€” itâ€™s not magic.
Itâ€™s the power of **recurrence** â€” machines learning to remember, one word at a time.

